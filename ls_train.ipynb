{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKQaX5wVO8Uc"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from util.train import embed_train_model\n",
    "from util.wordvec_load import LoadGlove, get_glove_embeddings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSUxM72mPMCN"
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 5\n",
    "nclass = {\"ag_news\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSj7K1TEPOag"
   },
   "outputs": [],
   "source": [
    "train = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m14RuekmsdDR"
   },
   "outputs": [],
   "source": [
    "def get_train(orig, full, per):\n",
    "  random.seed(42)\n",
    "\n",
    "  new_data = []\n",
    "  labels = orig[\"label\"].values\n",
    "  idx = 0\n",
    "  for x, y in zip(orig[\"text\"].values, full[\"text\"].values):\n",
    "    length = len(x.split())\n",
    "    s_range = list(range(0, length, 1))\n",
    "    choices = random.sample(s_range, int(per*length))\n",
    "    temp1 = x.split()\n",
    "    temp2 = y.split()\n",
    "    new_sent = []\n",
    "    for i in range(length):\n",
    "      try:\n",
    "        if i in choices:\n",
    "          new_sent.append(temp2[i])\n",
    "        else:\n",
    "          new_sent.append(temp1[i])\n",
    "      except IndexError:\n",
    "        continue\n",
    "    new_data.append({\"text\":\" \".join(new_sent), \"label\":labels[idx]})\n",
    "    idx += 1\n",
    "  return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCuyrPC8ujps"
   },
   "outputs": [],
   "source": [
    "train_path = \"data/ls_dropout_ag_news.csv\"\n",
    "dim = 300\n",
    "glove_path = \"glove.6B.300d.txt\" # Must download!\n",
    "orig = pd.read_csv(\"data/ag_news_preprocessed_train.csv\").sample(frac=0.5, random_state=42)\n",
    "df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_mOWEA1Pc-j",
    "outputId": "20c607d3-5131-403a-c981-4af114ed216a"
   },
   "outputs": [],
   "source": [
    "for p in [0.25, 0.5, 0.75, 1]:\n",
    "  if str(p) in train:\n",
    "    continue\n",
    "\n",
    "  print(p)\n",
    "\n",
    "  tdf = get_train(orig, df, p)\n",
    "  X = tdf['text'].values\n",
    "  y = tdf['label'].values\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  num_classes = nclass[\"ag_news\"]\n",
    "\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(X)\n",
    "  vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "  wv_model = LoadGlove(glove_path)\n",
    "  embedding_matrix = get_glove_embeddings(embeddings_index=wv_model, dim=dim, tokenizer=tokenizer)\n",
    "\n",
    "  y_train = to_categorical(y_train)\n",
    "  y_test = to_categorical(y_test)\n",
    "\n",
    "  training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "  maxlen = 500\n",
    "  training_padded = pad_sequences(training_sequences, maxlen=maxlen)\n",
    "\n",
    "  testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "  testing_padded = pad_sequences(testing_sequences, maxlen=maxlen)\n",
    "\n",
    "  X_train, X_test = training_padded, testing_padded\n",
    "\n",
    "  accuracies = []\n",
    "  model_savepath = \"models/\"\n",
    "  for i in range(NUM_TRAIN):\n",
    "      model, _ = embed_train_model(model_savepath, num_classes, embedding_matrix, X_train, y_train, X_test, y_test, vocab_size, maxlen, dim)\n",
    "      _, accuracy = model.evaluate(testing_padded, y_test)\n",
    "      accuracies.append(accuracy)\n",
    "  train[p] = {}\n",
    "  train[p][\"accs\"] = accuracies\n",
    "  train[p][\"mean\"] = np.mean(np.array(accuracies))\n",
    "  train[p][\"std\"] = np.std(np.array(accuracies))\n",
    "  print(\"{}: {}\".format(p, np.mean(np.array(accuracies))))\n",
    "\n",
    "  with open(\"ls_train.json\", 'w') as out:\n",
    "      json.dump(train, out, indent=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
